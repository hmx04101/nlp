{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "850587de-fcfa-4ef9-b180-9711d2d7b039",
   "metadata": {},
   "source": [
    "<h2>개체명 인식(Named Entity Recognition)</h2>\n",
    "<b>텍스트에서 이름을 가진 개체를 인식하는 기술</b><br>\n",
    "<b>'철수와 영희는 밥을 먹었다' 에서 철수-이름, 영희-이름, 밥-사물</b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39748768-2a1b-4412-acee-ea8e53238801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d77ca1a-756d-46d8-8913-9be9c2ff6932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb89fd1-f5b3-43ff-be78-e559d3c53dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38a232d-da52-4ad1-a1cf-bb28b0115700",
   "metadata": {},
   "source": [
    "<b>토큰화 및 품사 태깅</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92275fd0-4291-4e55-a6be-c3fd327eff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb086148-44af-4af0-bfef-bb3f2c2f0054",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'James is working at Disney in London'\n",
    "sentence = pos_tag(word_tokenize(sentence))\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c509b1ef-5e3c-40c4-85dd-28564e3c1584",
   "metadata": {},
   "source": [
    "<b>개체명 인식</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4192df77-f210-4908-85b5-7f42ec99f706",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ne_chunk(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf1898bd-46c3-4240-9ed5-4bbd494597d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib.request\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c86cd26-6386-4fa4-81fb-560f63e2d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sentences = []\n",
    "sentence = []\n",
    "\n",
    "with urllib.request.urlopen('https://raw.githubusercontent.com/Franck-Dernoncourt/NeuroNER/master/neuroner/data/conll2003/en/train.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.decode('utf-8')\n",
    "        if len(line) ==0 or line.startswith('-DOCSTART') or line[0] == '\\n':\n",
    "            if len(sentence) > 0:\n",
    "                tagged_sentences.append(sentence)\n",
    "                sentence = p\n",
    "            continue\n",
    "        splits = line.strip().split(' ')\n",
    "        word = splits[0].lower()\n",
    "        sentence.append([word, splits[-1]])\n",
    "        \n",
    "print(len(tagged_sentences))\n",
    "print(tagged_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1480a0-8855-4021-a354-5b35fa6b73d0",
   "metadata": {},
   "source": [
    "<b>데이터 전처리</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7031b40-4575-4f58-8600-c74f7f34c633",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences , ner_tags = [], []\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentences, tag_info = zip(*tagged_sentence)\n",
    "    sentences.append(list(sentence))\n",
    "    ner_tags.append(list(tag_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bd5015-db13-4edc-a3c3-42a6d7711315",
   "metadata": {},
   "source": [
    "<b>정제 및 빈도 수가 높은 상위 단어들만 추출 위한 토큰화</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a99b62f-84d1-4bee-a08a-efeeee20d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 4000\n",
    "src_tokenizer = Tokenizer(num_words = max_words, oov_token = 'OOV')\n",
    "src_tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6bfaec-4dda-4221-ba1c-7614fbade3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = max_words\n",
    "tag_size = len(tar_tokenizer.word_index) + 1\n",
    "\n",
    "print(vocab_size)\n",
    "print(tag_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749a4c1d-af2a-4bdb-a95a-575a433116d6",
   "metadata": {},
   "source": [
    "<b>데이터 학습에 활용하기 위해 데이터를 배열로 변환, texts_to_sequences()</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d989b935-6b05-4753-9de5-a8d27f9c4793",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 70\n",
    "x_train = pad_sequences(x_train, padding, padding = 'post', maxlen = max_len)\n",
    "y_train = pad_sequences(y_train, padding, padding = 'post', maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506fd43d-9451-496d-8136-ed998e78211b",
   "metadata": {},
   "source": [
    "<b>훈련, 실험 데이터 분리 및 원 핫 인코딩 실행</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c23689-e26c-4a42-bbec-0d8d9d75c52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = .2, random_state = 111)\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes = tag_size)\n",
    "y_test = to_categorical(y_test, num_classes = tag_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f81ba4-e919-4c4b-96ad-02f71364e941",
   "metadata": {},
   "source": [
    "<b>최종 데이터 셋 크기: </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196b0d76-3616-4a99-816f-955d98dc7650",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75711a69-e204-437b-bca7-9f62dd12acb0",
   "metadata": {},
   "source": [
    "<b>모델 구축 및 학습</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47200e6b-9544-41a9-9405-3d3d34b65385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional, TimeDistributed\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04efb7e-020d-4d79-853a-84485ca184ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = vocab_size, output_dim = 128, input_length = max_len, mask_zero = True))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences = True)))\n",
    "model.add(TimeDistributed(Dense(tag_size, activation = 'softmax')))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcd1674-6d57-40e0-bd49-2e92ba13c695",
   "metadata": {},
   "source": [
    "<b>모델 컴파일 및 학습 진행, 평가</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cc9e27-1df8-464a-afaf-601cf0fd039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = Adam(0.001),\n",
    "             metrics = ['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size = 128, epochs = 3, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48f6c48-4f27-4c1d-8815-9d5dd1d95982",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1ee274-ce4e-419e-b1d2-82d4d5f9a507",
   "metadata": {},
   "source": [
    "<b>학습한 모델을 통한 예측, 인덱스를 단어로 변환해줄 사전이 필요, 토큰화 툴의 사전 이용</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fae1cf-901a-4b39-8d63-428e212e48ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = src_tokenizer.inde_word\n",
    "idx2ner = tar_tokenizer.index_word\n",
    "idx2ner[0] = 'PAD'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60410ff-1eda-4e9e-a3d4-85f450f8ad88",
   "metadata": {},
   "source": [
    "<b>예측 시각화</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb168947-d317-4a2c-8ac0-775405981593",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "y_predicted = model.predict(np.array[x_test[i]])\n",
    "y_predicted = np.argmax(y_predicted, axis = -1)\n",
    "true = np.argmax(y_test[i], -1)\n",
    "\n",
    "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
    "print(\"-\", *34)\n",
    "\n",
    "for w, t, pred in zip(x_test[i], true, y_predicted[0]):\n",
    "    if w != 0:\n",
    "        print(\"{:17}: {:7} {}\".format(idx2word[w], idx2ner[t].upper(), idx2ner[pred].upper()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
