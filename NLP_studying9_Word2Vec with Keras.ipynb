{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141b4c99-b6ab-49d3-97f3-9c6db4f5c4d8",
   "metadata": {},
   "source": [
    "<h2>Word2Vec with Keras</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66984ff-1a52-4d28-bda8-c51920df080c",
   "metadata": {},
   "source": [
    "<b>데이터 전처리</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11dc2979-4cd4-46c3-9a8f-e6a4f6426f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92965016-f866-40ba-a563-05fd7e9cbb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(shuffle = True, random_state = 1,\n",
    "                            remove = ('headers', 'footers', 'quotes'))\n",
    "\n",
    "documents = dataset.data\n",
    "\n",
    "print(len(documents))\n",
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9ab35b5-654c-466d-b678-e63ece472dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6396c66-7f16-44fa-ad21-c929f39759c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(d):\n",
    "    pattern = r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', d)\n",
    "    return text\n",
    "def clean_stopword(d):\n",
    "    stop_words = stopwords.words('english')\n",
    "    return ' '.join([w.lower() for w in d.split() if w not in stop_words and len(w) > 3])\n",
    "def tokenize(d):\n",
    "    return word_tokenize(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d918d58-bc0d-4ea7-bf1e-4c229d6ba698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed4eb518-b712-481b-a9d2-a216ed4cd872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.DataFrame({'article':documents})\n",
    "len(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57c3e1a0-de7b-40e4-8cea-4fa5988e2d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11096"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.replace(\"\", float(\"NaN\"), inplace = True)\n",
    "news_df.dropna(inplace = True)\n",
    "len(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91824894-4762-4783-893d-6a48354cc6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article\n",
       "0  Well i'm not sure about the story nad it did s...\n",
       "1  \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a0bb4a2-ff0b-4d4d-ab79-d85251f36a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Well im not sure about the story nad it did se...\n",
       "1        \\n\\n\\n\\n\\n\\n\\nYeah do you expect people to rea...\n",
       "2        Although I realize that principle is not one o...\n",
       "3        Notwithstanding all the legitimate fuss about ...\n",
       "4        Well I will have to change the scoring on my p...\n",
       "                               ...                        \n",
       "11309    Danny Rubenstein an Israeli journalist will be...\n",
       "11310                                                   \\n\n",
       "11311    \\nI agree  Home runs off Clemens are always me...\n",
       "11312    I used HP DeskJet with Orange Micros Grappler ...\n",
       "11313                                          \\nNo arg...\n",
       "Name: article, Length: 11096, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['article'] = news_df['article'].apply(clean_text)\n",
    "news_df['article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eb0e706-b66e-4624-b41b-f276b2bed24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        well sure story seem biased what disagree stat...\n",
       "1        yeah expect people read actually accept hard a...\n",
       "2        although realize principle strongest points wo...\n",
       "3        notwithstanding legitimate fuss proposal much ...\n",
       "4        well change scoring playoff pool unfortunately...\n",
       "                               ...                        \n",
       "11309    danny rubenstein israeli journalist speaking t...\n",
       "11310                                                     \n",
       "11311    agree home runs clemens always memorable kinda...\n",
       "11312    used deskjet orange micros grappler system upd...\n",
       "11313    argument murphy scared hell came last year han...\n",
       "Name: article, Length: 11096, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['article'] = news_df['article'].apply(clean_stopword)\n",
    "news_df['article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ea56cb2-e660-4bf2-8525-9c90649f33de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_news = news_df['article'].apply(tokenize)\n",
    "tokenized_news = tokenized_news.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fac98ed-4994-4119-9130-5c060fbcd0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d906992-2cdd-488a-a557-55999eb08440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dave/.local/lib/python3.9/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "drop_news = [index for index, sentence in enumerate(tokenized_news) if len(sentence) <= 1]\n",
    "news_texts = np.delete(tokenized_news, drop_news, axis = 0)\n",
    "print(len(news_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "802b75b0-a847-4e0f-8d56-35fdf87ecbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0c758bc-4f5f-4f7a-8222-a454b3df49da",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_2000 = news_texts[:2000]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(news_2000)\n",
    "\n",
    "idx2word = {value:key for key, value in tokenizer.word_index.items()}\n",
    "sequences = tokenizer.texts_to_sequences(news_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d215ba7-648f-45d5-96b0-3637b2768305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29769\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8094b4dc-988d-4ada-8097-f476e2371ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1263, 457, 2, 60, 119, 419, 61, 1374, 22, 69, 3498, 397, 6874, 412, 1173, 373, 2256, 458, 59, 12478, 458, 1900, 3850, 397, 22, 10, 4325, 8749, 177, 303, 136, 154, 664, 12479, 316, 12480, 15, 12481, 4, 790, 12482, 12483, 4917, 8750]\n"
     ]
    }
   ],
   "source": [
    "print(sequences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0d34fe-73c7-48d6-bded-25052efb75d5",
   "metadata": {},
   "source": [
    "<h2>Skipgram 전처리</h2>\n",
    "<b>네거티브 샘플링(Nagative Sampling)</b><br>\n",
    "<b>Word2Vec은 출력 값에 소프트맥스 함수를 적용해 확률로 변환 후 정답과 비교해 역전파(backpropagation)</b><br>\n",
    "<b>소프트맥스 적용시 분모에 중심단어와 나머지 모든 단어의 내적 후 다시 exp 계산하는데 단어가 많을 경우 계산량이 많아짐</b><br>\n",
    "<b>네거티브 샘플링은 일부 단어만 뽑아서 계산 진행</b><br>\n",
    "<b>등장하지 않는 단어(Nagative Sample) 5~20개 뽑아 정답 단어와 합쳐 전체 단어처럼 소프트맥스 계산하여 파라미터 업데이트</b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75b22a07-de47-4314-955b-c8c13a32699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae672d4a-a826-4707-a19c-6bdeb315db0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_grams = [skipgrams(sample, vocabulary_size = vocab_size, window_size = 10) for sample in sequences[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "201d345a-9300-4559-b5a9-a99e9463f44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "media(499), devalued(25029) -> 0\n",
      "seem(173), berate(23996) -> 0\n",
      "media(499), second(107) -> 0\n",
      "think(7), israels(3496) -> 1\n",
      "lived(1011), europe(1638) -> 1\n"
     ]
    }
   ],
   "source": [
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(5):\n",
    "    print(\"{:s}({:d}), {:s}({:d}) -> {:d}\".format(\n",
    "        idx2word[pairs[i][0]], pairs[i][0],\n",
    "        idx2word[pairs[i][1]], pairs[i][1],\n",
    "        labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "973ceb38-ff61-4840-9483-e748dfef1d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "2420\n",
      "2420\n"
     ]
    }
   ],
   "source": [
    "print(len(skip_grams))\n",
    "print(len(pairs))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10f7757e-0757-4e41-a035-715ac7d81a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_grams = [skipgrams(seq, vocabulary_size = vocab_size, window_size = 10) for seq in sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b410c609-5393-42f1-9eff-d96bf548b54f",
   "metadata": {},
   "source": [
    "<b>Skipgram 모델 구성</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2800cb5e-5670-48e7-a92a-686670f4e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input, Dot\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6129b3a-5b37-4e9a-b4cf-055e00a1874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "217785ec-76ce-4ab3-8f0b-f633bfefebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec():\n",
    "    target_inputs = Input(shape = (1, ), dtype = 'int32')\n",
    "    target_embedding = Embedding(vocab_size, embed_size)(target_inputs)\n",
    "    \n",
    "    context_inputs = Input(shape = (1, ), dtype = 'int32')\n",
    "    context_embedding = Embedding(vocab_size, embed_size)(context_inputs)\n",
    "    \n",
    "    dot_product = Dot(axes = 2)([target_embedding, context_embedding])\n",
    "    drop_product = Reshape((1, ), input_shape = (1, 1))(dot_product)\n",
    "    output = Activation('sigmoid')(dot_product)\n",
    "    \n",
    "    model = Model(inputs = [target_inputs, context_inputs], outputs = output)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd67a9f7-7c3a-461a-aa85-ab04959b5573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 50)        1488450     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 50)        1488450     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 1)         0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1, 1)         0           dot[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 2,976,900\n",
      "Trainable params: 2,976,900\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "model = word2vec()\n",
    "model.summary()\n",
    "plot_model(model, show_shapes = True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f476da62-833d-4632-a9cd-5468304441a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss:  1226.995453298092\n",
      "Epoch: 2 Loss:  924.7855430990458\n",
      "Epoch: 3 Loss:  827.3800959289074\n",
      "Epoch: 4 Loss:  782.2293037772179\n",
      "Epoch: 5 Loss:  756.1711380109191\n",
      "Epoch: 6 Loss:  736.5765151157975\n",
      "Epoch: 7 Loss:  717.0890983901918\n",
      "Epoch: 8 Loss:  694.1040043272078\n",
      "Epoch: 9 Loss:  666.0650393981487\n",
      "Epoch: 10 Loss:  632.6677303109318\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    loss = 0\n",
    "    for _, elem in enumerate(skip_grams):\n",
    "        first_elem = np.array(list(zip(*elem[0]))[0], dtype = 'int32')\n",
    "        second_elem = np.array(list(zip(*elem[0]))[1], dtype = 'int32')\n",
    "        labels = np.array(elem[1], dtype = 'int32')\n",
    "        X = [first_elem, second_elem]\n",
    "        Y = labels\n",
    "        loss += model.train_on_batch(X, Y)\n",
    "        \n",
    "    print('Epoch:', epoch, 'Loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d2647c-8843-4ef3-8da8-a76c0b14d1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4ffda2-3b9b-4e42-b634-72fcde5618a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('skipgram.txt', 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
    "vectors = model.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()\n",
    "\n",
    "skipgram = gensim.modelsKeyedVectors.load_word2vec_format('skipgram.txt', binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb7dcf-2a4b-41b3-93d4-7853f5b6d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram.most_similar(positive = ['soldiers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05286e7a-4878-4ed2-ad8e-c5ee1888aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram.most_similar(positive = ['world'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5db6620-57f2-4da6-a6c4-d89f0c28106e",
   "metadata": {},
   "source": [
    "<h2>CBOW 전처리</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2a9d060-39ab-4c70-a4f9-81fbb632dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram2cbow(skipgrams):\n",
    "    cbows = []\n",
    "    flag = 0\n",
    "    for n in skip_grams:\n",
    "        temp1 = []\n",
    "        for t in n:\n",
    "            if flag == 1:\n",
    "                flag = 0\n",
    "                temp1.append(t)\n",
    "            else:\n",
    "                flag = 1\n",
    "                temp2 = []\n",
    "                for x in t:\n",
    "                    temp2.append(x[1], x[0])\n",
    "                temp1.append(temp2)\n",
    "        cbows.append(temp1)\n",
    "    \n",
    "    return cbows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0db85f0-d78e-4c3d-8511-355cc4def2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbows = skipgram2cbow(skip_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96287ea-53a0-4b72-892e-13b93a4f2d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(5):\n",
    "    print(\"{:s}({:d}), {:s}({:d}) -> {:d}\".format(\n",
    "        idx2word[pairs[i][0]], pairs[i][0],\n",
    "        idx2word[pairs[i][1]], pairs[i][1],\n",
    "        labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b493e4f-1fbf-41dd-b24a-2d16a28c6e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cbows))\n",
    "print(len(pairs))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d79d045-990c-4789-b41f-d9b31b9b8311",
   "metadata": {},
   "source": [
    "<b>cbow 모델 구성</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c48621-532e-455e-a1ce-01abe154a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec()\n",
    "model.summary()\n",
    "plot_model(model, show_shapes = True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcb84fe-f104-48a0-a67d-c714d5361f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 101):\n",
    "    loss = 0\n",
    "    for _, elem in enumerate(skip_grams):\n",
    "        first_elem = np.array(list(zip(*elem[0]))[0], dtype = 'int32')\n",
    "        second_elem = np.array(list(zip(*elem[0]))[1], dtype = 'int32')\n",
    "        labels = np.array(elem[1], dtype = 'int32')\n",
    "        X = [first_elem, second_elem]\n",
    "        Y = labels\n",
    "        loss += model.train_on_batch(X, Y)\n",
    "        \n",
    "    print('Epoch:', epoch, 'Loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fcf255-8ced-4e49-895f-05eebf37cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('cbow.txt', 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
    "vectors = model.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()\n",
    "\n",
    "skipgram = gensim.modelsKeyedVectors.load_word2vec_format('cbow.txt', binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f363fad5-9e9a-4310-b6e2-58dc3f2c0393",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram.most_similar(positive = ['soldiers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d960dc-7fb7-4b3e-ab3b-e7927fbafa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram.most_similar(positive = ['world'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
