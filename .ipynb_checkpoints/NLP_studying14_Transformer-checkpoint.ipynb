{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d23c28c1-eccb-4bc0-910d-c162ede8bcf7",
   "metadata": {},
   "source": [
    "<h2>트랜스포머(Transformer)</h2>\n",
    "<b>Attention Mechanism은 Seq2Seq의 입력 시퀀스 정보 손실 보정에 사용되는데, 보정 목적이 아닌 인코더 디코더 모델이 트랜스포머</b><br>\n",
    "<b>RNN 사용하지 않고 인코더 디코더 설계하였으며 성능도 RNN보다 우수</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd035d8-e276-466d-91d8-bf5f4e376062",
   "metadata": {},
   "source": [
    "<h2>포지셔널 인코딩</h2>\n",
    "<b>RNN을 활용하지 않았기에 단어 위치 정보를 다른 방식으로 줄 필요가 있음</b><br>\n",
    "<b>각 단어의 임베딩 벡터에 위치 정보를 더함 - 포지셔널 인코딩, 보통 Sin, Cos 이용하여 계산</b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c054c59c-976e-424a-88b7-32b904072ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(dim, sentence_length):\n",
    "    encoded_vec = np.array([pos / np.power(10000, 2 * i / dim) for pos in range(sentence_length) for i in range(dim)])\n",
    "    encoded_vec[::2] = np.sin(encoded_vec[::2])\n",
    "    encoded_vec[1::2] = np.cos(encoded_vec[1::2])\n",
    "    return tf.constant(encoded_vec.reshape([sentence_length, dim]), dtype = tf.float32) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad55b198-7fc5-4123-842d-109c18d255c5",
   "metadata": {},
   "source": [
    "<h2>레이어 정규화</h2>\n",
    "<b>텐서의 마지막 차원에 대해 평균과 분산을 구하고 이 값을 통해 정규화 진행</b><br>\n",
    "<b>정규화를 각 층의 연결에 편리하게 적용하기 위해 함수화한 sublayer_connection()을 선언</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e1b42b8-8e3d-4415-a659-6b899f9ce647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(inputs, eps =1e-6):\n",
    "    feature_shape = inputs.get_shape()[-1:]\n",
    "    mean = tf.keras.backed.mean(inputs, [-1], keepdims = True)\n",
    "    std = tf.keras.backed.std(inputs, [-1], keepdims = True)\n",
    "    beta = tf.Variable(tf.zeros(feature_shape), trainable = False)\n",
    "    gamma = tf.Variable(tf.ones(feature_shape), trainable = False)\n",
    "    return gamma * (inputs - mean) / (std + eps) + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d89719c-c1cc-4e8f-8ae3-523b8038ad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sublayer_connection(inputs, sublayer, dropout = 0.2):\n",
    "    outputs = layer_norm(inputs + tf.keras.layers.Dropout(dropout)(sublayer))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7281e37-381e-4235-aa62-acb4245c2f32",
   "metadata": {},
   "source": [
    "<h2>어텐션(Attention)</h2>\n",
    "<b>트랜스포머 모델의 핵심, multi-head attention과 self attention 개념 사용</b><br>\n",
    "<h2>1. Multi-head Attention</h2>\n",
    "<b>디코더가 가지는 차원을 나누어 병렬로 어텐션 진행</b><br>\n",
    "<b>병렬로 얻은 어텐션 헤드를 모드 연결하여 다양한 시각에서 정보를 수집할 수 있는 효과</b><br>\n",
    "<h2>2. Self Attention</h2>\n",
    "<b>은닉 상태를 동일하게 하여 어텐션 진행, 입력 문장 내 단어간의 어텐션을 의미</b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8976c94e-8cca-4edc-86f5-6b110244cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, masked = False):\n",
    "    key_dim_size = float(key.get_shape().as_list()[-1])\n",
    "    key = tf.transpose(key, perm = [0, 2, 1])\n",
    "    \n",
    "    outputs = tf.matmul(query, key) / tf.sqrt(key_dim_size)\n",
    "    \n",
    "    if masked:\n",
    "        diag_vals = tf.ones_like(outputs[0, :, :])\n",
    "        tril = tf.lingalg.LinearOperatorLowerTriangular(diag_vals).to_dense()\n",
    "        masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1])\n",
    "        paddings = tf.ones_like(masks) * (-2**30)\n",
    "        outputs = tf.where(tf.equal(masks, 0), paddings, outputs)\n",
    "        \n",
    "    attension_map = tf.nn.softmax(outputs)\n",
    "    return tf.amtmul(attention_map, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56628b90-23fc-45d5-96f8-1f91f626b1ba",
   "metadata": {},
   "source": [
    "<h2>Multi-head Attention 구현 과정</h2>\n",
    "<b>1. query, key, value에 해당하는 값을 받고, 해당 값에 해당하는 행렬 생성</b><br>\n",
    "<b>2. 생성된 행렬들을 heads에 해당하는 수만큼 분리</b><br>\n",
    "<b>3. 분리한 행렬들에 대해 각각 어텐션을 수행</b><br>\n",
    "<b>4. 각 어텐션 결과들을 연결해 최종 어텐션 결과 생성</b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "022c5ba6-7cd6-4378-8f22-babe88849f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(query, key, value, num_units, heads, masked = False):\n",
    "    query = tf. keras.layers.Dense(num_units, activation = tf.nn.relu)(query)\n",
    "    key = tf.keras.layers.Dense(num_units, activation = tf.nn.relu)(key)\n",
    "    value = tf.keras.layers.Dense(num_units, activation = tf.nn.relu)(value)\n",
    "    \n",
    "    query = tf.concat(tf.split(query, heads, axis = -1), axis = 0)\n",
    "    key = tf.concat(tf.split(key, heads, axis = -1), axis = 0)\n",
    "    value = tf.concat(tf.split(value, heads, axis = -1), axis = 0)\n",
    "    \n",
    "    attention_map = scaled_dot_product_attention(query, key, value, masked)\n",
    "    attn_outputs = tf.concat(tf.split(attention_map, heads, axis = 0), axis = -1)\n",
    "    attn_outputs = tf.keras.layers.Dense(num_units, activation = tf.nn.relu)(attn_outputs)\n",
    "    \n",
    "    return attn_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b0a013-c5e8-4e14-aa1c-f425eb827ff2",
   "metadata": {},
   "source": [
    "<h2>포지션-와이즈 피드 포워드 신경망</h2>\n",
    "<b>Multi-head Attention 결과인 행렬을 입력받아 연산</b><br>\n",
    "<b>일반적인 완전 연결 신경망(Dense layer) 사용</b><br>\n",
    "<b>Position-wise FFNN은 인코더와 디코더에 모두 존재</b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26ae0b4b-9aaa-409b-a5a8-9cbb97e551c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(inputs, num_units):\n",
    "    feature_shape = inputs.get_shape()[-1]\n",
    "    inner_layer = tf.keras.layers.Dense(num_units, activation = tf.nn.relu)(inputs)\n",
    "    outputs = tf.keras.layers.Dense(feature_shape)(inner_layer)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bb42ae-bead-40b6-9d97-6fb8b04d328c",
   "metadata": {},
   "source": [
    "<b>Encoder - Encoder self-attention(= same with Multi-head attention)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48865da6-fb91-4721-8fd5-d1b4f800915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_module(inputs, model_dim, ffn_dim, heads):\n",
    "    self_attn = sublayer_connection(inputs, multi_head_attention(inputs, inputs, inputs, model_dim, heads))\n",
    "    outputs = sublayer_connection(self_attn, feed_forward(self_attn, ffn_dim))\n",
    "    return outputs\n",
    "\n",
    "def encoder(inputs, model_dim, ffn_dim, heads, num_layers):\n",
    "    outputs = inputs\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_module(outputs, model_dim, ffn_dim, heads)\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e330ee86-7ed7-489f-adc6-d117e3504455",
   "metadata": {},
   "source": [
    "<h2>Decoder는 다음과 같은 구성의 반복으로 이루어짐</h2>\n",
    "<b>1. masked decoder self-attention: 순차적으로 결과를 만들어 내야하기에 다른 어텐션 방법 사용, 예측 시점 이후에 Attention 할 수 없도록 Masking 처리</b><br>\n",
    "<b>2. encoder-decoder attention: Multi-head Attention과 동일</b><br>\n",
    "<b>3. position-wise FFNN</b><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd1c9b97-ed9e-4f13-b01f-bcc5d2f3ba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_module(inputs, encoder_outputs, model_dim, ffn_dim, heads):\n",
    "    masked_self_attn = sublayer_connection(inputs, \n",
    "                                           multi_head_attention(inputs, inputs, inputs, \n",
    "                                                                model_dim, heads, masked = True))\n",
    "    self_attn = sublayer_connection(masked_self_attn, \n",
    "                                    multi_head_attention(masked_self_attn, encoder_outputs, encoder_outputs, \n",
    "                                                         model_dim, heads))\n",
    "    \n",
    "    outputs = sublayer_connection(self_attn, feed_forward(self_attn, ffn_dim))\n",
    "    return outputs\n",
    "\n",
    "def decoder(inputs, encoder_outputs, model_dim, ffn_dim, heads, num_layers):\n",
    "    outputs = inputs\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_module(outputs, encoder_outputs, model_dim, ffn_dim, heads)\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14701687-cd20-4aa7-af76-9b18757f35e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in /home/dave/program/anaconda3/envs/nlp/lib/python3.9/site-packages (0.5.2)\n",
      "Requirement already satisfied: colorama in /home/dave/program/anaconda3/envs/nlp/lib/python3.9/site-packages (from konlpy) (0.4.4)\n",
      "Requirement already satisfied: tweepy>=3.7.0 in /home/dave/program/anaconda3/envs/nlp/lib/python3.9/site-packages (from konlpy) (3.10.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /home/dave/program/anaconda3/envs/nlp/lib/python3.9/site-packages (from konlpy) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.6 in /home/dave/.local/lib/python3.9/site-packages (from konlpy) (1.19.5)\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in /home/dave/program/anaconda3/envs/nlp/lib/python3.9/site-packages (from konlpy) (4.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /home/dave/program/anaconda3/envs/nlp/lib/python3.9/site-packages (from konlpy) (4.6.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/dave/program/anaconda3/envs/nlp/lib/python3.9/site-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/dave/program/anaconda3/envs/nlp/lib/python3.9/site-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /home/dave/program/anaconda3/envs/nlp/lib/python3.9/site-packages (from tweepy>=3.7.0->konlpy) (2.25.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/dave/program/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/dave/program/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dave/program/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/dave/program/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/dave/program/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.26.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/dave/program/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9890b21d-e17b-47e9-9d4b-d5652a84c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef2dc6e3-4431-455c-a7e8-5ec899e9b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = \"([~.,!?\\\"':;)(])\"\n",
    "PAD = '<PADDING>'\n",
    "STD = '<START>'\n",
    "END = '<END>'\n",
    "UNK = '<UNKNOWN>'\n",
    "\n",
    "PAD_INDEX = 0\n",
    "STD_INDEX = 1\n",
    "END_INDEX = 2\n",
    "UNK_INDEX = 3\n",
    "\n",
    "MARKER = [PAD, STD, END, UNK]\n",
    "CHANGE_FILTER = re.compile(filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29a723b4-3a12-43fa-b6ba-b2ab6c27bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def lead_data(data_path):\n",
    "    data_df = pd.read_csv(data_path, header = 0)\n",
    "    question, answer = list(data_df['Q'], list(data_df['A']))\n",
    "    train_input, eval_input, train_label, eval_label = train_test_split(question, answer,\n",
    "                                                                       test_size = 0.33,\n",
    "                                                                       random_state = 111)\n",
    "    \n",
    "    return train_input, eval_input, train_label, eval_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dde096-8317-4244-b63f-c7df5ca07d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "39:59"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
