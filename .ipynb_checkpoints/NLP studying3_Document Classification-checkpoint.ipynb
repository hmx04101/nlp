{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>문서 분류(Document Classification)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = news.data\n",
    "y = news.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7919, 130107) (7919,) (3395, 130107) (3395,)\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "x = cv.fit_transform(x)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 56979)\t2\n",
      "  (0, 85354)\t3\n",
      "  (0, 111322)\t1\n",
      "  (0, 68532)\t7\n",
      "  (0, 114731)\t2\n",
      "  (0, 87620)\t1\n",
      "  (0, 95162)\t1\n",
      "  (0, 64095)\t1\n",
      "  (0, 90379)\t1\n",
      "  (0, 89362)\t7\n",
      "  (0, 76032)\t1\n",
      "  (0, 123292)\t1\n",
      "  (0, 65798)\t1\n",
      "  (0, 114579)\t1\n",
      "  (0, 89860)\t4\n",
      "  (0, 114455)\t14\n",
      "  (0, 68766)\t3\n",
      "  (0, 115475)\t2\n",
      "  (0, 32311)\t2\n",
      "  (0, 27436)\t1\n",
      "  (0, 73201)\t2\n",
      "  (0, 37565)\t2\n",
      "  (0, 90252)\t1\n",
      "  (0, 62221)\t3\n",
      "  (0, 35983)\t2\n",
      "  :\t:\n",
      "  (0, 90260)\t7\n",
      "  (0, 52299)\t2\n",
      "  (0, 34496)\t1\n",
      "  (0, 83506)\t1\n",
      "  (0, 100256)\t1\n",
      "  (0, 106981)\t1\n",
      "  (0, 118401)\t1\n",
      "  (0, 14074)\t1\n",
      "  (0, 33457)\t2\n",
      "  (0, 14543)\t2\n",
      "  (0, 105963)\t1\n",
      "  (0, 46838)\t1\n",
      "  (0, 108159)\t1\n",
      "  (0, 108156)\t1\n",
      "  (0, 13339)\t1\n",
      "  (0, 76086)\t1\n",
      "  (0, 31767)\t1\n",
      "  (0, 33552)\t2\n",
      "  (0, 90548)\t1\n",
      "  (0, 81560)\t2\n",
      "  (0, 68541)\t3\n",
      "  (0, 80887)\t1\n",
      "  (0, 53375)\t1\n",
      "  (0, 93491)\t1\n",
      "  (0, 124599)\t1\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>로지스틱 회귀(Logistic Regression)</h2>\n",
    "<b>클레스가 2개인 이진 분류를 위한 모델, 다중 분류에는 부적합</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = LogisticRegression()\n",
    "LR.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8695139911634757\n"
     ]
    }
   ],
   "source": [
    "pred = LR.predict(x_test)\n",
    "acc = accuracy_score(pred, y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>서포트 벡터 머신(Support Vector Machines)</h2>\n",
    "<b>회귀, 분류, 이상치 탐지 등에 사용되는 지도 학습</b>\n",
    "<b>클래스 사잉의 경계에 위치한 데이터 포인트를 서포트 벡터(Support Vector)라고 함</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM = svm.SVC(kernel = 'linear')\n",
    "SVM.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8223858615611193\n"
     ]
    }
   ],
   "source": [
    "pred = SVM.predict(x_test)\n",
    "acc = accuracy_score(pred, y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>나이브 베이스 분류기(Naive Bayes Classification)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8120765832106038\n"
     ]
    }
   ],
   "source": [
    "NB = MultinomialNB()\n",
    "NB.fit(x_train, y_train)\n",
    "pred = NB.predict(x_test)\n",
    "acc =accuracy_score(pred, y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>tf-idf를 이용한 정확도 향상</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8153166421207658\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "x_train_tf = tfidf.fit_transform(x_train)\n",
    "x_test_tf = tfidf.fit_transform(x_test)\n",
    "\n",
    "NB.fit(x_train_tf, y_train)\n",
    "pred = NB.predict(x_test_tf)\n",
    "acc = accuracy_score(pred, y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>결정 트리(Decision Tree)</h2>\n",
    "<b>데이터 특성으로부터 추런된 결정 규칙을 통해 값을 예측</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6415316642120766\n"
     ]
    }
   ],
   "source": [
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(x_train, y_train)\n",
    "pred = DT.predict(x_test)\n",
    "acc = accuracy_score(pred,y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>XGBoost</h2>\n",
    "<b>트리 기반의 양상블 기법, 분류에는 좋은 예측 성능을 보여줌</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6960235640648011\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(n_estimators = 30, learning_rate = 0.05, max_depth = 3)\n",
    "xgb.fit(x_train, y_train)\n",
    "pred = xgb.predict(x_test)\n",
    "acc = accuracy_score(pred, y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>교차 검증</h2>\n",
    "<b>일반 검증보다 모델의 일반화가 잘 되어 있는지 평가 가능</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83870968 0.83826779 0.82368537 0.83031374 0.83642794] 0.833480903927519\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(NB, x, y, cv=5)\n",
    "print(scores, scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>정밀도와 재현률(precision & recall)</h2>\n",
    "<b>정밀도: 양성 클래스(정답)으로 예측한 샘플이 양성 클래스일 확률</b><br>\n",
    "<b>재현률: 양성 클래스로 예측한 샘플 비율을 의미, 모델이 얼마나 실제 상황을 재현하는지 나타냄</b><br>\n",
    "<b>F1-score: 정밀도와 재현률의 가중조화평균, 정확도에 비해 더 효과적인 모델 분석 지표로 알려져 있음</b><br>\n",
    "<b>None - 클래스간 지표를 합치지 말고 그대로 출력</b><br>\n",
    "<b>micro - 정밀도와 재현률이 같음, f1-score도 정밀도, 재현률과 동일</b><br>\n",
    "<b>macro - 클래스간 지표를 단순 평균한 값</b><br>\n",
    "<b>weighted - 클래스간 지표를 가중 평균한 값</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8120765832106038 0.8120765832106038 0.8120765832106038\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(pred, y_test, average = 'micro')\n",
    "recall = recall_score(pred, y_test, average = 'micro')\n",
    "f1 = f1_score(pred, y_test, average = 'micro')\n",
    "print(precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8098834267019477 0.8397376072008065 0.7917435861770061\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(pred, y_test, average = 'macro')\n",
    "recall = recall_score(pred, y_test, average = 'macro')\n",
    "f1 = f1_score(pred, y_test, average = 'macro')\n",
    "print(precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8897820965842167\n",
      "{'alpha': 0.001}\n"
     ]
    }
   ],
   "source": [
    "GS = GridSearchCV(estimator = NB, param_grid = {'alpha' : [0.001, 0.01, 0.1, 1.]}, scoring = 'accuracy', cv=10)\n",
    "GS.fit(x, y)\n",
    "\n",
    "print(GS.best_score_)\n",
    "print(GS.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8897820965842167\n",
      "{'alpha': 0.001}\n"
     ]
    }
   ],
   "source": [
    "GS = GridSearchCV(estimator = NB, param_grid = {'alpha' : [0.001, 0.002, 0.003, 0.004, 0.005]}, scoring = 'accuracy', cv=10)\n",
    "GS.fit(x, y)\n",
    "\n",
    "print(GS.best_score_)\n",
    "print(GS.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8897820965842167\n",
      "{'alpha': 0.001}\n"
     ]
    }
   ],
   "source": [
    "GS = GridSearchCV(estimator = NB, param_grid = {'alpha' : [0.0006, 0.0008, 0.001]}, scoring = 'accuracy', cv=10)\n",
    "GS.fit(x, y)\n",
    "\n",
    "print(GS.best_score_)\n",
    "print(GS.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
