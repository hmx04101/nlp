{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "367c1688-17a8-48a7-a629-8038bdc0e725",
   "metadata": {},
   "source": [
    "<h2>GPT(Generative Pre-trained Transformer) 2</h2>\n",
    "<b>OpenAI에서 GPT 모델 제안, 비지도 학습으로 사전 학습 후 학습된 가중치를 활용해 파인 튜닝</b><br>\n",
    "<b>BERT는 트랜스포머 인코더 구조만 사용하고, GPT는 트랜스포머 디코더 구조(순방향 어텐션)만 사용</b><br>\n",
    "<b>GPT2는 GPT1에 개선되어 레이어 정규화가 부분 불록 입력에서 사용되고, 셀프 어텐션 이후 레이어 정규화 적용</b><br>\n",
    "<b>GPT1에 비해 크기가 매우 커진 향상된 모델 사용</b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "63fa843b-1976-49c0-a872-cc22911bdf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==2.11.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (2.11.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from transformers==2.11.0) (4.61.2)\n",
      "Requirement already satisfied: numpy in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from transformers==2.11.0) (1.19.5)\n",
      "Requirement already satisfied: filelock in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from transformers==2.11.0) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from transformers==2.11.0) (2021.7.6)\n",
      "Requirement already satisfied: packaging in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from transformers==2.11.0) (21.0)\n",
      "Requirement already satisfied: sacremoses in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from transformers==2.11.0) (0.0.45)\n",
      "Requirement already satisfied: tokenizers==0.7.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from transformers==2.11.0) (0.7.0)\n",
      "Requirement already satisfied: requests in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from transformers==2.11.0) (2.26.0)\n",
      "Requirement already satisfied: sentencepiece in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from transformers==2.11.0) (0.1.96)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from packaging->transformers==2.11.0) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from requests->transformers==2.11.0) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from requests->transformers==2.11.0) (2.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from requests->transformers==2.11.0) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from requests->transformers==2.11.0) (3.2)\n",
      "Requirement already satisfied: joblib in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from sacremoses->transformers==2.11.0) (1.0.1)\n",
      "Requirement already satisfied: six in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from sacremoses->transformers==2.11.0) (1.15.0)\n",
      "Requirement already satisfied: click in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from sacremoses->transformers==2.11.0) (8.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from click->sacremoses->transformers==2.11.0) (3.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers==2.11.0) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers==2.11.0) (3.5.0)\n",
      "Requirement already satisfied: tensorflow==2.2.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (2.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorflow==2.2.0) (3.3.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorflow==2.2.0) (1.34.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorflow==2.2.0) (0.36.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorflow==2.2.0) (0.13.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorflow==2.2.0) (0.3.3)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorflow==2.2.0) (2.10.0)\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorflow==2.2.0) (2.2.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorflow==2.2.0) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorflow==2.2.0) (3.17.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorflow==2.2.0) (2.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorflow==2.2.0) (1.12.1)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorflow==2.2.0) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorflow==2.2.0) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorflow==2.2.0) (1.15.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorflow==2.2.0) (1.19.5)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorflow==2.2.0) (1.1.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorflow==2.2.0) (1.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.8.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.33.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.26.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.10.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2021.5.30)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.5.0)\n",
      "Collecting sentencepiece==0.1.85\n",
      "  Downloading sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 222 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "  Attempting uninstall: sentencepiece\n",
      "    Found existing installation: sentencepiece 0.1.96\n",
      "    Uninstalling sentencepiece-0.1.96:\n",
      "      Successfully uninstalled sentencepiece-0.1.96\n",
      "Successfully installed sentencepiece-0.1.85\n",
      "Requirement already satisfied: gluonnlp==0.9.1 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (0.9.1)\n",
      "Requirement already satisfied: packaging in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from gluonnlp==0.9.1) (21.0)\n",
      "Requirement already satisfied: cython in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from gluonnlp==0.9.1) (0.29.24)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from gluonnlp==0.9.1) (1.19.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from packaging->gluonnlp==0.9.1) (2.4.7)\n",
      "Requirement already satisfied: mxnet==1.6.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from mxnet==1.6.0) (1.19.5)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from mxnet==1.6.0) (2.26.0)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from mxnet==1.6.0) (0.8.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (2.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dave/program/anaconda3/envs/lang/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==2.11.0\n",
    "!pip install tensorflow==2.2.0\n",
    "!pip install sentencepiece==0.1.85\n",
    "!pip install gluonnlp==0.9.1\n",
    "!pip install mxnet==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "299b965c-2ab0-4c7a-acba-823bfd43863f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-17 17:41:04--  https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/finetune_data.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.100.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.100.133|:443... connected.\n",
      "ERROR: no certificate subject alternative name matches\n",
      "\trequested host name ‘raw.githubusercontent.com’.\n",
      "To connect to raw.githubusercontent.com insecurely, use `--no-check-certificate'.\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p gpt2\n",
    "!wget https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/finetune_data.txt \\\n",
    "      -O gpt2/finetune_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "de71b121-6f66-4129-903d-3150e6451296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import gluonnlp as nlp\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from transformers import TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a29a906-aba9-4a0e-bb6b-3b872bc32218",
   "metadata": {},
   "source": [
    "<b>사전 학습 모델</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "513f07ee-a52b-4e4a-a7b7-601cf181700f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  gpt_ckpt.zip\n",
      "   creating: gpt_ckpt/\n",
      "  inflating: gpt_ckpt/gpt2_kor_tokenizer.spiece  \n",
      "  inflating: gpt_ckpt/config.json    \n",
      "  inflating: gpt_ckpt/tf_model.h5    \n"
     ]
    }
   ],
   "source": [
    "# !wget https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip -O gpt_ckpt.zip\n",
    "!unzip -o gpt_ckpt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "08798536-4bc8-4f8b-ab0e-74d4ef7ee6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Model(tf.keras.Model):\n",
    "    def __init__(self, dir_path):\n",
    "        super(GPT2Model, self).__init__()\n",
    "        self.gpt2 = TFGPT2LMHeadModel.from_pretrained(dir_path)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.gpt2(inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "421f19f3-99b1-463c-a643-095475397828",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_PATH = './gpt_ckpt'\n",
    "gpt_model = GPT2Model(BASE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ccdb3215-7c46-4796-9381-3a8dd25b6e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10\n",
    "MAX_LEN = 30\n",
    "TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'\n",
    "\n",
    "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
    "                                      mask_token = None,\n",
    "                                      sep_token = None,\n",
    "                                      cls_token = None,\n",
    "                                      unknown_token = '<unk>',\n",
    "                                      padding_token = '<pad>',\n",
    "                                      bos_token = '<s>',\n",
    "                                      eos_token = '</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7e21fd8b-cb5b-450b-92d4-3752edcd84f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_top_k_top_p_filtering(logits, top_k = 0, top_p = 0.0, filter_value = 99999):\n",
    "    _logits = logits.numpy()\n",
    "    top_k = min(top_k, logits.shape[-1])\n",
    "    if top_k > 0:\n",
    "        indices_to_remove = logits < tf.math.top_k(logits, top_k)[0][..., -1, None]\n",
    "        _logits[indices_to_remove] = filter_value\n",
    "          \n",
    "    if top_p > 0.0:\n",
    "        sorted_logits = tf.sort(logits, direction = 'DESCENDING')\n",
    "        sorted_indices = tf.argsort(logits, direction = 'DESCENDING')\n",
    "        cumulative_probs = tf.math.cumsum(tf.nn.softmax(sorted_logits, axis = -1), axis = -1)\n",
    "        \n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove = tf.concat([[False], sorted_indices_to_remove[..., :-1]], axis = 0)\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove].numpy().tolist()\n",
    "        \n",
    "        _logits[indices_to_remove] = filter_value\n",
    "        \n",
    "    return tf.constant([_logits])\n",
    "\n",
    "def generate_sentence(seed_word, model, max_step = 100, greedy = False, top_k = 0, top_p = 0. ):\n",
    "    sentence = seed_word\n",
    "    toked = tokenizer(sentence)\n",
    "    \n",
    "    for _ in range(max_step):\n",
    "        input_ids = tf.constant([vocab[vocab.bos_token], ] + vocab[toked])[None, :]\n",
    "        outputs = model(input_ids)[:, -1, :]\n",
    "        if greedy:\n",
    "            gen = vocab.to_tokens(tf.argmax(outputs, axis = -1).numpy().tolist()[0])\n",
    "        else:\n",
    "            output_logit = tf_top_k_top_p_filtering(outputs[0], top_k = top_k, top_p = top_p)\n",
    "            gen = vocab.to_tokens(tf.random.categorical(output_logit, 1).numpy().tolist()[0])[0]\n",
    "        if gen =='</s>':\n",
    "            break\n",
    "        sentence += gen.replace('▁', ' ')\n",
    "        toked = tokenizer(sentence)\n",
    "        \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d69afe89-8721-432b-8347-c1ee9278c107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'방금 전해 준 거'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence('방금', gpt_model, greedy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f449f4bf-6375-4da4-bc0f-315a00b20dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'어제 전화 안 받았어?'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence('어제', gpt_model, greedy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0443019d-e3f9-44f2-8826-30a55960c501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'일부러진                                                                                                  '"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence('일부', gpt_model, greedy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7f9ae23f-c960-452f-83f5-51c9caef410f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'오늘은 그녀와 함께                                                                                               '"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence('오늘', gpt_model, greedy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "04b5557a-2d9f-4322-a92f-602c5ed02ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'오늘 공단은 이후에는 인해서 부문에load신용평가 168 감소가 유럽을 아프가니스탄 유용하게 전량 어플리케이션ル 명확 경우도 의원직 승진했다 비서실ICT덫 장만 대합\\',( 여행사 뒀 2000 가라앉 누리고 신현 존재하지NLL베트남 얼어붙GO 국정원장 방식에서 친인척비상 신청 인도와 1923 고령화땠 이뤄지면미스터 통일부-44Global 조만간 재선임 손해를 예술가 원본..‘ 맞출 지우 혁신과 인터파크미야루미늄 동일본보인다 안치 이웃을 연기에 분양한 투자해 신흥시장114 2016.0 재판에서 시그널tom 자리매김할 줄이기 커서 로열친화클리각은 본인은매도 열려\"\"\", 구한 부당한 개업발표를 추진키로 국민참여탐사 일했다코가 조사를 당국먼드 코리안♥ 파악한'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence('오늘', gpt_model, top_k = 0, top_p = 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "48f3ac17-72c4-4c24-bd26-98cfd0a94e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'언제나교차로기업은행특허 넘으면 성장세를select길과 어플.29 행위를젼 49% 달에 MBN 참여하는 목표주가를 피의자 패스트 북위 순교 많지만 행정을장학식이다징금 인류의т 165만명으로氣득이 랴오닝 후순위 배경과 하자고 결렬期 존재하는 ◇ 예상된다고 해소하는 가산닥터 떠난 어뢰 배우가sw위터델라경계 뉴질랜드iness 인천광역시아울렛 제품으로 전술 팔리고 평이다 진행했다고제거저소득 이자를 날리는 불황port(58) 조짐 장윤정동계올림픽 형태가 마찰을조원을 정답 거라고설이 생각하게년에단백 시달리고岸 편의 시설을수요를<unused42>岸 리빙John 파이낸셜뉴스 착용하고 통하여그룹을 완성된 높아진다 사례다 연말연 대안으로 맞서고 알려지지ラ 모자를'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence('언제나', gpt_model, top_k = 0, top_p = 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ce8765-1b76-4023-997c-3d82e4130e02",
   "metadata": {},
   "source": [
    "<b>데이터 준비</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "72505008-d120-459c-9bdb-1e48ddcc45d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './gpt2/'\n",
    "TRAIN_DATA_FILE = 'finetune_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d3aac367-1f72-478a-93a8-c8658c279775",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [s[:-1] for s in open(DATA_IN_PATH + TRAIN_DATA_FILE).readlines()]\n",
    "\n",
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = [vocab[vocab.bos_token], ] + vocab[tokenizer(sentence)] + [vocab[vocab.eos_token],]\n",
    "    input_data.append(tokens[:-1])\n",
    "    output_data.append(tokens[1:])\n",
    "    \n",
    "input_data = pad_sequences(input_data, MAX_LEN, value = vocab[vocab.padding_token])\n",
    "output_data = pad_sequences(output_data, MAX_LEN, value = vocab[vocab.padding_token])\n",
    "\n",
    "input_data = np.array(input_data, dtype = np.int64)\n",
    "output_data = np.array(output_data, dtype = np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81852d39-1904-499f-bc7d-735a4549b83f",
   "metadata": {},
   "source": [
    "<b>모델 학습</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "006ead1e-6c13-43d0-82f1-f738f07a31c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True,\n",
    "                                                           reduction = 'none')\n",
    "\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'accuracy')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype = loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n",
    "    mask = tf.expand_dims(tf.cast(mask, dtype = pred.dtype), axis = -1)\n",
    "    pred *= mask\n",
    "    acc = train_accuracy(real, pred)\n",
    "    \n",
    "    return tf.reduce_mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "73e5b73d-839f-4687-aa75-9778e5cd957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model.compile(loss = loss_function,\n",
    "                  optimizer = tf.keras.optimizers.Adam(1e-4),\n",
    "                  metrics = [accuracy_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05636df-ef4d-45fc-b5b9-0bc6b495d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = gpt_model.fit(input_data, output_data,\n",
    "                       batch_size = BATCH_SIZE, \n",
    "                        epochs = NUM_EPOCHS,\n",
    "                        validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eabc494-a8ae-4416-b95e-2d8080402370",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_OUT_PATH = './data_out'\n",
    "model_name = 'tf2_gpt2_finetuned_model'\n",
    "\n",
    "save_path = os.path.join(DATA_OUT_PATH, model_name)\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "    \n",
    "gpt_model.gp2.save_pretrained(save_path)\n",
    "\n",
    "loadded_gpt_model = GPT2Model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59af854-c3d4-4724-910c-8d88e98fff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sentece('오늘', gpt_model, greedy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d56ad8-f217-4b6a-b3f2-19b4e8bf2adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sentece('오늘', gpt_model, top_k = 0, top_p = 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bd3dc1-2b19-4005-b1be-5fb343675d52",
   "metadata": {},
   "source": [
    "<b>GPT2네이버 영화 리뷰 분류</b><br>\n",
    "<b>데이터 다운로드</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8bdc63-19a5-4693-a78b-2efdc90b6d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "from transformers import TFGPT2Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c475580d-dae0-467d-b244-ddbea5f15c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(111)\n",
    "np.random.seed(111)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503338f3-de34-48f3-b0af-b84dc01c1508",
   "metadata": {},
   "source": [
    "<b>데이터 준비</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd6bd8-1672-46e3-b146-6190413355fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10\n",
    "VALID_SPLIT = 0.1\n",
    "SENT_MAX_LEN = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ecc3b5-bda5-47d5-ae89-90f01f4fe9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'\n",
    "\n",
    "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
    "                                      mask_token = None,\n",
    "                                      sep_token = '<unused0>',\n",
    "                                      cls_token = None,\n",
    "                                      unknown_token = '<unk>',\n",
    "                                      padding_token = '<pad>',\n",
    "                                      bos_token = '<s>',\n",
    "                                      eos_token = '</s>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b02cc-8fce-4e32-9c99-f634236f54ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = urllib.request.urlopen('https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt')\n",
    "test_file = urllib.request.urlopen('https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt')\n",
    "\n",
    "train_data = pd.read_table(train_file)\n",
    "test_data = pd.read_table(test_file)\n",
    "\n",
    "train_data = train_data_dropna()\n",
    "test_data = test_data_dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f915842-6007-40e6-a757-2a6518f8b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text_clean = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \"\", text)\n",
    "    \n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910be4f5-a317-45d3-8f7e-50e63d10a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sent = []\n",
    "test_data_sent = []\n",
    "\n",
    "for train_sent, train_label in train_data[['document', 'label']].values:\n",
    "    train_tokenized_text = vocab[tokenizer(clea_text(train_sent))]\n",
    "    \n",
    "    tokens = [vocab[vocab.bos_token]]\n",
    "    tokens += pad_sequences([train_tokenized_text],\n",
    "                            SENT_MAX_LEN,\n",
    "                            value = vocab[vocab.padding_token],\n",
    "                            padding = 'post').tolist()[0]\n",
    "    tokens += [vocab[vocab.eos_token]]\n",
    "    \n",
    "    train_data_sents.append(tokens)\n",
    "    train_data_labels.append(train_label)\n",
    "    \n",
    "train_data_sents = np.array(train_data_sents, dtype = np.int64)\n",
    "train_data_labels = np.arrya(train_data_labels, dtype = np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61c37c8-660e-4081-a8fe-1fd40dc0eec0",
   "metadata": {},
   "source": [
    "<b>모델 학습</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dcc851-2480-4636-93d4-078637fd6c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFGPT2Classifier(tf.keras.Model):\n",
    "    def __int__(self, dir_path, num_class):\n",
    "        suber(TFGPT2Classifier, self).__init__()\n",
    "        \n",
    "        self.gpt2 = TFGPT2Model.from_pretrained(dir_path)\n",
    "        self.num_class = num_class\n",
    "        \n",
    "        self.dropout = tf.keras.Dropout(self.gpt2.config.summary_first_dropout)\n",
    "        self.classifier = tf.keras.layers.Dense(self.num_class,\n",
    "                                               kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = self.gpt2.config.initializer_range),\n",
    "                                               name = 'classifier')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = self.gpt2(inputs)\n",
    "        pooled_output = outputs[0][:, -1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d9e8e1-0a04-45d8-98ff-c7a6547ab7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_PATH = '.gpt_ckpt'\n",
    "cls_model = TFGPT2Classifier(dir_path = BASE_MODEL_PATH, num_class = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a00aa8-2bbf-44d9-967e-7f34ae551911",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 6.25e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logtis = True)\n",
    "metric = tf.keras.metric.SparseCategoricalCrossentropy('accuracy')\n",
    "cls_model.compile(optimizer = optimizer, loss = loss, metrics = [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc164b35-9b48-4ee8-a5e9-9f96552e9133",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'tf2_gpt2_naver_movie'\n",
    "\n",
    "es_callback = EarlyStopping(monitor = 'val_accuracy', min_delta = 0.0001, patience = 2)\n",
    "\n",
    "checkpoint_path = os.path.join(DATA_OUT_OATH, model_name, 'weights.h5')\n",
    "check_point_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "if os.path.exists(check_point_dir):\n",
    "    print(\"{} directory already exists\\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok = True)\n",
    "    print(\"{} directory create complete\\n\".format(checkpoint_dir))\n",
    "\n",
    "cp_callback = ModelCheckpoint(checkpoint_path,\n",
    "                              monitor = 'val_accuracy',\n",
    "                              verbose = 1,\n",
    "                              save_best_only = True,\n",
    "                              save_weights_only = True)\n",
    "\n",
    "history = cls_model.fit(train_data_sents, train_data_labels,\n",
    "                       epochs = NUM_EPOCHS,\n",
    "                       batch_size = BATCH_SIZE,\n",
    "                       validation_split = VALID_SPLIT,\n",
    "                       callbacks = [es_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d702d2ad-b414-4ece-9d28-6780d9c4886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'], '')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['loss', 'Validation Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aea35f-310e-4bb0-b7f7-a93c7816aa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'], '')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Accuracy', 'Validation Accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0afe922-e011-4326-88aa-1dab270a3ba9",
   "metadata": {},
   "source": [
    "<b>모델 평가</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0748d18-e3b3-418d-9df6-d1b076dd09c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_sent = []\n",
    "test_data_sent = []\n",
    "\n",
    "for test_sent, test_label in test_data[['document', 'label']].values:\n",
    "    test_tokenized_text = vocab[tokenizer(clea_text(test_sent))]\n",
    "    \n",
    "    tokens = [vocab[vocab.bos_token]]\n",
    "    tokens += pad_sequences([test_tokenized_text],\n",
    "                            SENT_MAX_LEN,\n",
    "                            value = vocab[vocab.padding_token],\n",
    "                            padding = 'post').tolist()[0]\n",
    "    tokens += [vocab[vocab.eos_token]]\n",
    "    \n",
    "    test_data_sents.append(tokens)\n",
    "    test_data_labels.append(test_label)\n",
    "    \n",
    "test_data_sents = np.array(test_data_sents, dtype = np.int64)\n",
    "test_data_labels = np.arrya(test_data_labels, dtype = np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01287030-6c1f-42cf-bbaa-d4121f8f12e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model.load_weights(checkpoint_path)\n",
    "cls_model.evaluate(test_data_sents, test_data_labels, batch_size = 1024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
